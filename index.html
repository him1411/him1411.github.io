<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Himanshu Gupta</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }
        a {
            color: #0066cc;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
Himanshu Gupta
          
 
 
          
           Applied Scientist at Amazon working on development of foundation models for Amazon Business. My main research contributions include showcasing sample efficiency of instruction tuned language models, mathematical robustness, synthetic data generation, and aspect based sentiment analysis. Previously, I worked on creating a multi-lingual 7B foundation model focused on Indic languages at Krutrim.
          
 
 
          
 
 
          
           <a href="mailto:himanshugupta@email.com">Email</a>  /  <a href="https://scholar.google.com/citations?hl=en&user=ydjuhxsAAAAJ">Google Scholar</a>  / 
           <a href="https://www.linkedin.com/in/himanshugupta/">LinkedIn</a>  /  
           <a href="https://twitter.com/himanshugupta">Twitter</a>
          
 
 
         	
          
           
          
         
 
 
 
      	
          Research
          
           I am an Applied Scientist at Amazon where I work on development of foundation models for Amazon Business. My main research contributions are showcasing sample efficiency of instruction tuned language models, mathematical robustness, synthetic data generation and aspect based sentiment analysis. I completed my Bachelors from BITS Pilani and masters majoring in Computer Science (Thesis Track) from Arizona State University. My thesis was on "Sample efficiency of Instruction Tuned Models" supervised by Prof. Chitta Baral.
           <br><br>
           My research interests include Large Language Models, Enhancing Pretraining Corpora, Instruction Tuning, and Direct Preference Optimization. I collaborate with Dr. Swaroop Mishra on topics such as Instruction tuning, Synthetic dataset creation and mathematical reasoning. My strength lies in generating new ideas and I am fortunate to collaborate with a diverse set of awesome researchers.
           <br><br>
           I recently interned at Krutrim where I created a multi-lingual 7B foundation model focused on Indic languages and developed a new tokenizer for the same! It keeps the vocabulary concise at around 100,000 words while still understanding most words. We also built a multilingual large language model (LLM) that understands 10 Indic languages (<a href="https://www.youtube.com/watch?v=5BhN0Qopt_0&t=2203s">Media Coverage</a>). This involved gathering pretraining corpora of nearly 2 trillion tokens and training the LLM. To further improve its performance, I worked on generating supervised finetuning data. Finally, with the help of vLLM and TensorRT, we deployed the LLM for efficient use with high throughput and low latency.
          
 
 
         
 
 
 
      	
          
 
 
          
 
 
          
 
 
         	
          
           Selected Papers
          
         
	
          
 
 
          
 
 
          
 
 
         	
          
           <a href="https://drive.google.com/file/d/1VGnfqJeP-8fM0vBe4a8maDmi5KDSp-Sl/view?usp=sharing">PolyMATH: A Challenging Multi-modal Mathematical Reasoning Benchmark</a> 
          
          
 
 
           Himanshu Gupta, Shreyas Verma, Ujjwala Anantheswaran, Kevin Scaria, Mihir Parmar, Swaroop Mishra, Chitta Baral
          <br>preprint
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2406.15444">Investigating the Robustness of LLMs on Math Word Problems</a>  
 
 
          Ujjwala Anantheswaran, Himanshu Gupta, Kevin Scaria, Shreyas Verma, Chitta Baral, Swaroop Mishra
          <br>preprint
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2310.17876">TarGEN: Targeted Data Generation with Large Language Models</a>  
 
 
          Himanshu Gupta, Kevin Scaria, Ujjwala Anantheswaran, Shreyas Verma, Mihir Parmar, Saurabh Arjun Sawant, Chitta Baral, Swaroop Mishra
          <br>COLM 2024
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2305.16357">EDM3: Event Detection as Multi-task Text Generation</a>  
 
 
          Ujjwala Anantheswaran, Himanshu Gupta, Mihir Parmar, Kuntal Kumar Pal, Chitta Baral
          <br>SEM NAACL 2024
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2302.08624">InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis</a>  
 
 
          Kevin Scaria, Himanshu Gupta, Siddharth Goyal, Saurabh Arjun Sawant, Swaroop Mishra, Chitta Baral
          <br>NAACL 2024
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://www.atipriya.com/files/papers/varbert_oakland24.pdf">"Len or index or count, anything but v1": Predicting Variable Names in Decompilation Output with Transfer Learning</a>  
 
 
          Kuntal Kumar Pal, Ati Priya Bajaj, Pratyay Banerjee, Audrey Dutcher, Mutsumi Nakamura, Zion Leonahenahe Basque, Himanshu Gupta, Saurabh Arjun Sawant, Ujjwala Anantheswaran, Yan Shoshitaishvili, Adam Doup√©, Chitta Baral, Ruoyu Wang
          <br>IEEE S&P 2023
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2305.05079">A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution</a>  
 
 
          Neeraj Varshney, Himanshu Gupta, Eric Robertson, Bing Liu, Chitta Baral
          <br>ACL 2023
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2210.07471">"John is 50 years old, can his son be 65?" Evaluating NLP Models' Understanding of Feasibility</a>  
 
 
          Himanshu Gupta, Neeraj Varshney, Swaroop Mishra, Kuntal Kumar Pal, Saurabh Arjun Sawant, Kevin Scaria, Siddharth Goyal, Chitta Baral
          <br>EACL 2023
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2109.08079">Context-NER: Contextual Phrase Generation at Scale</a>  
 
 
          Himanshu Gupta, Shreyas Verma, Santosh Mashetty, Swaroop Mishra
          <br>NeurIPS ENLSP Workshop 2022
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2311.09564">LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks</a>  
 
 
          Mihir Parmar, Aakanksha Naik, Himanshu Gupta, Disha Agrawal, Chitta Baral
          <br>preprint
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
           <a href="https://arxiv.org/abs/2306.05539">Instruction Tuned Models are Quick Learners</a>  
 
 
          Himanshu Gupta, Saurabh Arjun Sawant, Swaroop Mishra, Mutsumi Nakamura, Arindam Mitra, Santosh Mashetty, Chitta Baral
          <br>preprint
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
          
           News
          
         
	
          
 
 
          
 
 
          
 
 
         	
          
           07.2024 - TarGEN accepted at COLM 2024
          
          <br>
 
           05.2024 - EDM3 accepted at SEM NAACL 2024
          <br>
 
           03.2024 - InstructABSA accepted at NAACL 2024
          <br>
 
           01.2024 - Reached 100 citations
          <br>
 
           12.2023 - Joined Amazon as an Applied Scientist
          <br>
 
           12.2023 - Graduated from Arizona State University with Distinction
          <br>
 
           09.2023 - Reached 50 citations
          <br>
 
           08.2023 - Received 1500\$ merit scholarship from School of Computing and AI at ASU
          <br>
 
           07.2023 - Started 40hr co-op as Applied Scientist at <a href="https://olakrutrim.com/">Krutrim</a>
          <br>
 
           06.2023 - Paper accepted at ACL 2023
          <br>
 
           05.2023 - Started internship at Amazon Alexa
          <br>
 
           01.2022 - Started Masters in Computer Science at Arizona State University
          <br>
 
           07.2019 - Joined American Express AI Labs as a Research Engineer
          <br>
 
           06.2019 - Graduated from BITS Pilani
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
          
           Education
          
         
	
          
 
 
          
 
 
          
 
 
         	
          
           01.2022 - 12.2023 - Masters (with Thesis) in Computer Science from Arizona State University
          
          <br>
 
           08.2015 - 07.2019 - B.E. with Hons. in Electrical and Electronics Engineering, BITS Pilani, India
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
          
           Experience
          
         
	
          
 
 
          
 
 
          
 
 
         	
          
           12.2023 - present - Applied Scientist at Amazon
          
          <br>
 
           08.2023 - 11.2023 - 40hr co-op as Applied Scientist LLM at <a href="https://olakrutrim.com/">Krutrim</a>
          <br>
 
           05.2023 - 07.2023 - Internship at Amazon Alexa
          <br>
 
           01.2022 - 05.2023 - Graduate Research Assistant at CogInt Labs, ASU under <a href="https://www.public.asu.edu/~cbaral/">Dr. Chitta Baral</a>
          <br>
 
           01.2019 - 12.2021 - AI Researcher at American Express AI Labs. Supervised by <a href="https://scholar.google.com/citations?user=dZQOltMAAAAJ&hl=en">Dr. Himanshu Shrad Bhatt</a>
          <br>
 
           01.2018 - 12.2018 - Research Intern at Covenant University under <a href="https://scholar.google.com/citations?user=AFjvjTgAAAAJ&hl=en">Dr. Sanjay Misra</a>
          <br>
 
           01.2018 - 12.2018 - Undergraduate Research Assistant at BITS Pilani under <a href="https://dblp.org/pid/158/5831.html">Dr. NL Bhanu Murthy</a>
 
 
         
	
          
 
 
          
 
 
          
 
 
         	
          
           Honors and Awards
          
         
	
          
 
 
          
 
 
          
 
 
         	
          
           08.2023 - Received Merit Scholarship of 1500\$ for exceptional acedemic performance at ASU
          
          <br>
 
           01.2022 - Received Tuition Waiver and additional monthly stipend as Research Assistant from Arizona State University
          <br>
 
           12.2021 - Platinum Genius Medal for filing 3 patents in American Express
          <br>
 
           03.2019 - Best Author Award at IEMECON 2019
          <br>
 
           04.2014 - KVPY Scholarship
 
 
         
 
 
 
      	
          
 
 
          Template from the awesome Jon
</body>
</html>
